#!/usr/bin/env python3

import os
import re
import json
import uuid
import fitz
import subprocess
import tempfile
from docx import Document
from tqdm import tqdm
from pdf2image import convert_from_path
import pytesseract
from dotenv import load_dotenv
from openai import OpenAI
from pathlib import Path
from dotenv import dotenv_values

# âœ… åˆå§‹åŒ–
load_dotenv()

root_dir = Path(__file__).resolve().parent.parent
config = dotenv_values(root_dir / ".env")

api_key = config.get("OPENAI_API_KEY")
embedding_model = config.get("OPENAI_EMBEDDING_MODEL")
llm_model = config.get("OPENAI_LLM_FIELD_EXTRACT_MODEL")
ie_model = None

INPUT_DIR = os.getenv("RESUME_DIR", "data/resumes")
OUTPUT_DIR = os.getenv("EXTRACTED_DIR", "data/resumes_extract_enhanced")
BLACKLIST = [
    "ä¸ªäººç®€å†", "ç®€å†", "æ±‚èŒ", "BOSSç›´è˜", "çŒè˜", "å®¢æˆ·åç§°", "é¡¹ç›®åç§°",
    "original", "standard", "çš„", "doc", "pdf", "docx", "é™„ä»¶"
]

openai_client = OpenAI(api_key=api_key)

# ========= OCR å…œåº• =========
def extract_via_ocr(file_path):
    try:
        print(f"ğŸ“¸ OCR è¯†åˆ«ä¸­ï¼š{os.path.basename(file_path)}")
        images = convert_from_path(file_path, dpi=300)
        results = [pytesseract.image_to_string(img, lang='chi_sim', config='--psm 6 --oem 3') for img in images]
        text_result = "\n".join([t.strip() for t in results if t.strip()])
        return text_result if len(text_result.strip()) >= 30 else None
    except Exception as e:
        print(f"âš ï¸ OCR å¤±è´¥: {e}")
        return None

# ========= æ–‡æœ¬æå– =========
def extract_text(file_path):
    ext = file_path.lower().split('.')[-1]
    if ext == "pdf":
        return extract_pdf_text(file_path)
    elif ext == "docx":
        text = extract_docx_text(file_path)
        if not text or len(text.strip()) < 30:
            pdf_path = convert_to_pdf(file_path)
            return extract_via_ocr(pdf_path)
        return text
    elif ext == "doc":
        return extract_doc_text(file_path)
    return None

def convert_to_pdf(docx_path):
    pdf_path = os.path.splitext(docx_path)[0] + ".pdf"
    try:
        subprocess.run(["soffice", "--headless", "--convert-to", "pdf", "--outdir", os.path.dirname(docx_path), docx_path], check=True)
        return pdf_path if os.path.exists(pdf_path) else docx_path
    except:
        return docx_path

def extract_pdf_text(file_path):
    try:
        doc = fitz.open(file_path)
        lines = []
        for page in doc:
            blocks = page.get_text("dict")["blocks"]
            for b in blocks:
                if "lines" in b:
                    for line in b["lines"]:
                        span_text = " ".join([span["text"] for span in line["spans"] if span.get("size", 0) > 8])
                        if span_text.strip():
                            lines.append(span_text.strip())
        result = "\n".join(lines)
        if len(result.strip()) < 300:
            return extract_via_ocr(file_path)
        return result
    except:
        return extract_via_ocr(file_path)

def extract_doc_text(file_path):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".txt") as temp_output:
            subprocess.run(["unoconv", "-f", "txt", "-o", temp_output.name, file_path], check=True)
            with open(temp_output.name, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
            os.remove(temp_output.name)
            if len(text.strip()) > 100:
                return text
            pdf_path = convert_to_pdf(file_path)
            return extract_via_ocr(pdf_path)
    except:
        return None

def extract_docx_text(file_path):
    try:
        doc = Document(file_path)
        paras = [p.text for p in doc.paragraphs if p.text.strip()]
        tables = []
        for table in doc.tables:
            for row in table.rows:
                row_text = " | ".join([cell.text.strip() for cell in row.cells if cell.text.strip()])
                if row_text:
                    tables.append(row_text)
        return "\n".join(paras + tables)
    except:
        return None

# ========= æ¸…æ´—å¢å¼º =========
def enhance_text(text):
    text = re.sub(r'\b[a-zA-Z0-9]{20,}\b', '', text)
    text = re.sub(r'(ç®€å†æ¥è‡ª|BOSSç›´è˜|çŒè˜|å‰ç¨‹æ— å¿§)[^\n]*', '', text)
    text = re.sub(r'(~{2,}|-{2,}|={2,}|\+{2,})', '', text)
    text = re.sub(r'\s{2,}', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

# ========= å­—æ®µæå– =========
def extract_fields(text):
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¿¡æ¯æŠ½å–ä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹ä¸­æ–‡ç®€å†æ–‡æœ¬ä¸­æå–å‡ºå€™é€‰äººçš„åŸºç¡€ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹å­—æ®µè¾“å‡º JSONï¼ˆæ— åˆ™å†™ nullï¼‰ï¼š

- å§“åï¼šä¼˜å…ˆä»æ–‡æœ¬å‰100å­—ç¬¦ä¸­æŸ¥æ‰¾å§“åï¼Œé¿å…é”™è¯¯æå–èŒä½åæˆ–æ ¼å¼è¯´æ˜ã€‚
- åº”è˜èŒä½ï¼šå¦‚"é”€å”®æ€»ç›‘""è¿è¥ç»ç†"ç­‰è§’è‰²èŒç§°ã€‚
- æ‰‹æœºå·ï¼šä¸­å›½å¤§é™†æ‰‹æœºå·ï¼Œ11ä½çº¯æ•°å­—ã€‚
- é‚®ç®±ï¼šåŒ…å« @ å­—ç¬¦ï¼Œå¸¸è§é‚®ç®±æ ¼å¼ã€‚

è¯·ä½ åªè¾“å‡ºæ ‡å‡† JSON æ ¼å¼ï¼Œä¸æ·»åŠ æ³¨é‡Šæˆ–è§£é‡Šã€‚

ç¤ºä¾‹ï¼š
{{
  "å§“å": "å¼ ä¸‰",
  "åº”è˜èŒä½": "äº§å“ç»ç†",
  "æ‰‹æœºå·": "13812345678",
  "é‚®ç®±": "zhangsan@example.com"
}}

ä»¥ä¸‹æ˜¯ç®€å†æ­£æ–‡ï¼š
{text[:3000]}
"""
    try:
        response = openai_client.chat.completions.create(
            model=llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=300
        )
        content = response.choices[0].message.content.strip()
        content = re.sub(r"^```(json)?|```$", "", content)
        return json.loads(content)
    except Exception as e:
        print(f"âš ï¸ å­—æ®µæå–å¤±è´¥: {e}")
        return {}

# ========= æ¨¡å—ç»“æ„åˆ†ç±»æå– =========
def classify_resume_sections(text):
    prompt = f"""
ä½ æ˜¯ä¸€ä½ç»“æ„åˆ†æåŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹ç®€å†å†…å®¹è¿›è¡Œåˆ†ç±»æ•´ç†ï¼Œå°†å…¶æ‹†åˆ†ä¸ºå¦‚ä¸‹æ¨¡å—ï¼š

- å²—ä½ç»å†ï¼šå…·ä½“å·¥ä½œå•ä½ã€æ—¶é—´ã€å²—ä½åŠèŒè´£ã€‚
- é¡¹ç›®ç»å†ï¼šå‚ä¸çš„é¡¹ç›®å†…å®¹ã€æˆæœã€èŒè´£ã€‚
- æ•™è‚²èƒŒæ™¯ï¼šå­¦æ ¡ã€å­¦å†ã€ä¸“ä¸šã€æ—¶é—´ã€‚
- æŠ€èƒ½äº®ç‚¹ï¼šæŠ€èƒ½åç§°ã€æŒæ¡ç¨‹åº¦ã€è¯ä¹¦ç­‰ã€‚
- è‡ªæˆ‘è¯„ä»·ï¼šè‡ªè¿°å‹å†…å®¹ï¼ŒåŒ…æ‹¬ä¼˜åŠ£åŠ¿ã€æ€§æ ¼ç­‰ã€‚

è¦æ±‚ï¼š

- ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªæ¨¡å—ã€‚
- æ¯ä¸ªæ¨¡å—åŒ…å«å­—æ®µï¼š"æ¨¡å—" å’Œ "å†…å®¹"ã€‚
- ä¸åŒ…å« markdownã€è§£é‡Šè¯´æ˜ã€é¢å¤–æ ‡ç­¾ã€‚
- æ¨¡å—å†…å®¹å°½é‡æç‚¼ä¸ºç®€æ´æ®µè½ï¼Œå¦‚æœ‰ç¼–å·/è¡¨æ ¼å¯åˆå¹¶ã€‚

ç¤ºä¾‹è¾“å‡ºï¼š
[
  {{
    "æ¨¡å—": "å²—ä½ç»å†",
    "å†…å®¹": "2020å¹´-2023å¹´åœ¨æŸç§‘æŠ€å…¬å¸æ‹…ä»»äº§å“ç»ç†ï¼Œè´Ÿè´£ç”µå•†å¹³å°è®¾è®¡ã€‚"
  }},
  {{
    "æ¨¡å—": "æ•™è‚²èƒŒæ™¯",
    "å†…å®¹": "2014å¹´-2018å¹´ï¼Œå¤æ—¦å¤§å­¦ï¼Œæœ¬ç§‘ï¼Œè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ã€‚"
  }}
]

ä»¥ä¸‹æ˜¯ç®€å†å†…å®¹ï¼š
{text[:3000]}
"""
    try:
        response = openai_client.chat.completions.create(
            model=llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=1000
        )
        content = response.choices[0].message.content.strip()
        content = re.sub(r"^```(json)?|```$", "", content)
        return json.loads(content)
    except Exception as e:
        print(f"âš ï¸ ç»“æ„è¯†åˆ«å¤±è´¥: {e}")
        return []

# ========= æ–‡ä»¶åæ¸…æ´— =========
def sanitize_filename_part(value: str) -> str:
    return re.sub(r'[\\/:*?"<>|]', '_', value or "null")

# ========= å§“å fallback =========
def extract_name_fallback(filename, text):
    base = os.path.splitext(os.path.basename(filename))[0]
    first_part = base.split('_')[0]  # é€šå¸¸æ˜¯å§“åéƒ¨åˆ†
    name_candidates = re.findall(r'[\u4e00-\u9fa5]{2,4}', first_part)
    for name in name_candidates:
        if name not in BLACKLIST:
            # æ¸…æ´—æ‰å°¾éƒ¨çš„"çš„"æˆ–"ç®€å†"ç­‰éäººåè¯
            name = re.sub(r'(çš„|ç®€å†)$', '', name)
            return name
    head_text = text[:50]
    name_candidates = re.findall(r'[\u4e00-\u9fa5]{2,4}', head_text)
    for name in name_candidates:
        if name not in BLACKLIST:
            name = re.sub(r'(çš„|ç®€å†)$', '', name)
            return name
    return "æœªçŸ¥å§“å"

# ========= ä¸»æµç¨‹ =========
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.pdf', '.doc', '.docx'))]
    success, fail = 0, 0

    for filename in tqdm(files, desc="ğŸ“„ æ­£åœ¨æå–ç®€å†"):
        try:
            path = os.path.join(INPUT_DIR, filename)
            text = extract_text(path)
            if not text or len(text.strip()) < 10:
                text = "[æ–‡ä»¶è¯»å–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º]"

            text = enhance_text(text)
            fields = extract_fields(text)

            # fallback: å§“åä¸èƒ½ä¸ºç©º
            if not fields.get("å§“å") or fields["å§“å"] == "null":
                fields["å§“å"] = extract_name_fallback(filename, text)

            if not fields.get("åº”è˜èŒä½"):
                fields["åº”è˜èŒä½"] = "æœªçŸ¥èŒä½"

            name = re.findall(r'[\u4e00-\u9fa5]{2,4}', filename)[0] if re.findall(r'[\u4e00-\u9fa5]{2,4}', filename) else "æœªçŸ¥å§“å"
            position = fields.get("åº”è˜èŒä½", "æœªçŸ¥èŒä½")
            name_clean = re.sub(r'[\\/:*?"<>|]', '_', name)
            position_clean = re.sub(r'[\\/:*?"<>|]', '_', position)
            output_name = f"{name_clean}_{position_clean}_{str(uuid.uuid4())[:8]}.txt"
            output_path = os.path.join(OUTPUT_DIR, output_name)

            with open(output_path, "w", encoding="utf-8") as f:
                f.write("=== å­—æ®µæå–ç»“æœ ===\n")
                f.write(json.dumps(fields, ensure_ascii=False, indent=2))
                f.write("\n\n=== åŸå§‹ç®€å†æ–‡æœ¬ ===\n")
                f.write(text)

            print(f"âœ… è¾“å‡ºå®Œæˆï¼š{output_name}")
            success += 1

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ï¼š{filename} | {e}")
            with open("failures.log", "a") as log:
                log.write(f"å¤„ç†å¤±è´¥: {filename} | {e}\n")
            fail += 1

    print(f"\nğŸ¯ æ€»æ•°ï¼š{len(files)} | æˆåŠŸï¼š{success} | å¤±è´¥ï¼š{fail}")

if __name__ == "__main__":
    main()