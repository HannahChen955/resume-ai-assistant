#!/usr/bin/env python3

import os
import uuid
import time
import json
import logging
import hashlib
import re
from typing import List
from tqdm import tqdm
from dotenv import load_dotenv
import dashscope
from dashscope import TextEmbedding
import requests
import weaviate

# âœ… å¼ºåˆ¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå¤‡ç”¨ï¼‰
load_dotenv()

# âœ… DashScope API Keyï¼ˆhardcodedï¼Œç”¨æˆ·æŒ‡å®šä¿ç•™ï¼‰
dashscope.api_key = "sk-1d92a7280052451c84509f57e1b44991"

# âœ… æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# âœ… ç¯å¢ƒå˜é‡é…ç½®
RESUMES_DIR = os.getenv("EXTRACTED_DIR", "data/resumes_extract_enhanced")
WEAVIATE_URL = os.getenv("WEAVIATE_URL", "http://localhost:8080")
WEAVIATE_CLASS = os.getenv("WEAVIATE_COLLECTION", "Candidates")
DASHSCOPE_EMBEDDING_MODEL = os.getenv("DASHSCOPE_EMBEDDING_MODEL", "text-embedding-v1")
NAMESPACE_UUID = uuid.UUID("12345678-1234-5678-1234-567812345678")
TOP_N = 5
MAX_CHUNK_LENGTH = 300

# âœ… è‡ªåŠ¨æ³¨å†Œ classï¼ˆvectorizer=noneï¼‰
def ensure_class_exists():
    url = f"{WEAVIATE_URL}/v1/schema"
    resp = requests.get(url)
    schema = resp.json()
    classes = [c['class'] for c in schema.get('classes', [])]
    if WEAVIATE_CLASS not in classes:
        logger.info("ğŸ“š Weaviate ä¸­æœªæ‰¾åˆ° %sï¼Œå‡†å¤‡è‡ªåŠ¨æ³¨å†Œ...", WEAVIATE_CLASS)
        payload = {
            "class": WEAVIATE_CLASS,
            "vectorizer": "none",
            "properties": [
                {"name": "filename", "dataType": ["text"]},
                {"name": "content", "dataType": ["text"]},
                {"name": "name", "dataType": ["text"]},
                {"name": "position", "dataType": ["text"]},
                {"name": "notes", "dataType": ["text[]"]},
            ]
        }
        resp = requests.post(url, json=payload)
        if resp.status_code == 200:
            logger.info("âœ… å·²è‡ªåŠ¨æ³¨å†Œ Weaviate class: %s", WEAVIATE_CLASS)
        else:
            logger.error("âŒ æ³¨å†Œ class å¤±è´¥: %s", resp.text)

# âœ… å‘é‡å‰æ–‡æœ¬æ¸…æ´—ï¼ˆä¸æˆªæ–­ï¼Œä»…å»é‡ç©ºè¡Œï¼‰
def clean_text_for_embedding(text: str) -> str:
    text = re.sub(r"\n{2,}", "\n", text)
    text = re.sub(r"[^\S\r\n]+", " ", text)
    return text.strip()

# âœ… è·å–å‘é‡ï¼ˆé€šä¹‰ DashScope å¼ºå¥ç‰ˆæœ¬ï¼‰
def get_embedding(text: str) -> list[float]:
    text = clean_text_for_embedding(text)
    logger.info(f"ğŸ” å‘é‡åŒ–æ–‡æœ¬é•¿åº¦: {len(text)}")
    try:
        response = TextEmbedding.call(
            model=DASHSCOPE_EMBEDDING_MODEL,
            input=text
        )
        logger.info(f"[DEBUG] DashScope åŸå§‹å“åº”: {response}")

        if isinstance(response, dict):
            embeddings = response.get("output", {}).get("embeddings", [])
            if embeddings and isinstance(embeddings[0], dict):
                vector = embeddings[0].get("embedding")
                logger.info(f"[DEBUG] å‘é‡ç»´åº¦: {len(vector) if isinstance(vector, list) else 'N/A'}")
                logger.info(f"[DEBUG] å‘é‡ç±»å‹: {type(vector)}")
                logger.info(f"[DEBUG] å‘é‡é¢„è§ˆ: {vector[:5] if isinstance(vector, list) else vector}")
                if vector and isinstance(vector, list) and all(isinstance(x, (float, int)) for x in vector):
                    logger.info("âœ… å‘é‡åŒ–æˆåŠŸï¼Œå‰5ç»´: %s", vector[:5])
                    return vector

        logger.error(f"âŒ å‘é‡æå–å¤±è´¥ï¼Œç»“æ„å¼‚å¸¸: {response}")
        return []
    except Exception as e:
        logger.exception(f"âŒ DashScope å¼‚å¸¸: {e}")
        return []

# âœ… æ–‡æœ¬åˆ†æ®µ
def chunk_text(text: str, max_length: int) -> List[str]:
    sentences = re.split(r'[\nã€‚ï¼ï¼Ÿ!\?]', text)
    chunks, current = [], ""
    for sentence in sentences:
        if len(current) + len(sentence) < max_length:
            current += sentence + "ã€‚"
        else:
            if current:
                chunks.append(current.strip())
            current = sentence + "ã€‚"
    if current:
        chunks.append(current.strip())
    return [c for c in chunks if len(c) > 10]

# âœ… è¯»å–ç®€å†æ–‡æœ¬
def load_resume_text(file_path: str) -> str:
    if file_path.endswith(".txt"):
        with open(file_path, 'r', encoding="utf-8") as f:
            return f.read()
    return ""

# âœ… æ‹†åˆ†ç»“æ„
def split_txt_sections(full_text: str):
    pattern = r"===\s*å­—æ®µæå–ç»“æœ\s*===\n(.*?)\n+===\s*æ¨¡å—ç»“æ„åˆ†ç±»ç»“æœ\s*===\n(.*?)\n+===\s*åŸå§‹ç®€å†æ–‡æœ¬\s*===\n(.*)"
    match = re.search(pattern, full_text, re.DOTALL)
    if match:
        fields_str, sections_str, raw_text = match.groups()
        return fields_str.strip(), sections_str.strip(), raw_text.strip()
    else:
        return "", "", full_text.strip()

# âœ… æ‹¼æ¥æ–‡æœ¬ç”¨äºå‘é‡åŒ–
def build_vector_text(fields_str: str, sections_str: str, raw_text: str) -> str:
    return f"""ã€å­—æ®µä¿¡æ¯ã€‘\n{fields_str}\n\nã€æ¨¡å—ç»“æ„åˆ†ç±»ã€‘\n{sections_str}\n\nã€åŸæ–‡è¡¥å……ã€‘\n{raw_text}"""

# âœ… æ£€æŸ¥å¯¹è±¡æ˜¯å¦å­˜åœ¨
def object_exists(resume_uuid: str) -> bool:
    url = f"{WEAVIATE_URL}/v1/objects/{WEAVIATE_CLASS}/{resume_uuid}"
    response = requests.get(url)
    return response.status_code == 200

# âœ… ä¸Šä¼ å¯¹è±¡
def upload_resume(filename: str, content: str, vector: List[float], resume_uuid: str, name: str = "", position: str = ""):
    logger.info(f"[DEBUG] upload_resume: filename={filename}, name={name}, position={position}, vectorå‰5ç»´={vector[:5] if vector else vector}")
    if not vector:
        logger.warning("âš ï¸ å‘é‡ä¸ºç©ºï¼Œè·³è¿‡ä¸Šä¼ : %s", filename)
        return

    payload = {
        "class": WEAVIATE_CLASS,
        "id": resume_uuid,
        "properties": {
            "filename": filename,
            "content": content,
            "name": name,
            "position": position,
            "notes": []
        },
        "vector": vector
    }

    logger.info("ğŸ§¾ å‡†å¤‡æ’å…¥å¯¹è±¡ payload:\n%s", json.dumps(payload, indent=2, ensure_ascii=False))

    url = f"{WEAVIATE_URL}/v1/objects"
    response = requests.post(url, json=payload)
    logger.info(f"[DEBUG] Weaviate å“åº”çŠ¶æ€: {response.status_code}, å“åº”ä½“: {response.text}")
    if response.status_code == 200:
        logger.info("âœ… æ’å…¥æˆåŠŸ: %s", filename)
    else:
        logger.error("âŒ æ’å…¥å¤±è´¥: %sï¼ŒçŠ¶æ€ç : %d", filename, response.status_code)
        try:
            logger.error("é”™è¯¯ä¿¡æ¯: %s", response.json())
        except:
            logger.warning("âš ï¸ å“åº”ä½“è§£æå¤±è´¥")

# âœ… ç”Ÿæˆ UUID
def generate_uuid_from_file(filename: str, content: str) -> str:
    hash_val = hashlib.sha256(content.encode('utf-8')).hexdigest()
    return str(uuid.uuid5(NAMESPACE_UUID, filename + hash_val))

# âœ… ä¸»å‡½æ•°
def index_resumes_topn():
    logger.info("ğŸ”§ å½“å‰ä½¿ç”¨é€šä¹‰æ¨¡å‹: %s", DASHSCOPE_EMBEDDING_MODEL)
    ensure_class_exists()

    if not os.path.exists(RESUMES_DIR):
        logger.error("âŒ ç®€å†ç›®å½•ä¸å­˜åœ¨: %s", RESUMES_DIR)
        return

    files = [f for f in os.listdir(RESUMES_DIR) if f.endswith(".txt")]
    if not files:
        logger.warning("âš ï¸ æ²¡æœ‰å‘ç°ç®€å†æ–‡ä»¶")
        return

    logger.info("ğŸ“„ å¼€å§‹å¤„ç†ç®€å†:")
    start = time.time()

    for filename in tqdm(files):
        file_path = os.path.join(RESUMES_DIR, filename)
        full_text = load_resume_text(file_path)
        if not full_text.strip():
            logger.warning("âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶: %s", filename)
            continue

        fields, sections, raw = split_txt_sections(full_text)
        name_match = re.search(r"å§“å[:ï¼š]\s*(.*)", fields)
        position_match = re.search(r"(ç›®æ ‡èŒä½|èŒä½)[:ï¼š]\s*(.*)", fields)
        name = name_match.group(1).strip() if name_match else ""
        position = position_match.group(2).strip() if position_match else ""
        logger.info(f"[DEBUG] è§£æåˆ° name: {name}, position: {position}, file: {filename}")

        merged_text = build_vector_text(fields, sections, raw)
        chunks = chunk_text(merged_text, MAX_CHUNK_LENGTH)
        selected_chunks = chunks[:TOP_N]
        if not selected_chunks:
            logger.warning("âš ï¸ æ— æœ‰æ•ˆæ®µè½: %s", filename)
            continue

        final_text = "\n".join(selected_chunks)
        resume_uuid = generate_uuid_from_file(filename, final_text)

        if object_exists(resume_uuid):
            logger.info("â© å·²å­˜åœ¨: %s", filename)
            continue

        try:
            vector = get_embedding(final_text)
            logger.info(f"ğŸ“ æœ€ç»ˆå‘é‡å‰5ç»´: {vector[:5] if vector else 'ç©ºå‘é‡'}")
            upload_resume(filename, final_text, vector, resume_uuid, name, position)
            time.sleep(0.3)
        except Exception as e:
            logger.exception("âŒ ä¸Šä¼ å¤±è´¥: %s", filename)

    elapsed = time.time() - start
    logger.info("âœ… æ‰€æœ‰ç®€å†å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ %.2fs", elapsed)

if __name__ == "__main__":
    index_resumes_topn()