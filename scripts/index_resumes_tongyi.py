#!/usr/bin/env python3

import os
import uuid
import time
import json
import logging
import hashlib
import re
from typing import List
from tqdm import tqdm
from dotenv import load_dotenv
import dashscope
from dashscope import TextEmbedding
import requests

# âœ… åŠ è½½ .env æ–‡ä»¶
load_dotenv()
dashscope.api_key = "sk-1d92a7280052451c84509f57e1b44991"

# âœ… æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# âœ… ç¯å¢ƒå˜é‡é…ç½®
RESUMES_DIR = os.getenv("EXTRACTED_DIR", "data/resumes_extract_enhanced")
WEAVIATE_URL = os.getenv("WEAVIATE_URL", "http://localhost:8080")
WEAVIATE_CLASS = os.getenv("WEAVIATE_COLLECTION", "Candidates")
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "")
DASHSCOPE_EMBEDDING_MODEL = os.getenv("DASHSCOPE_EMBEDDING_MODEL", "text-embedding-v1")
NAMESPACE_UUID = uuid.UUID("12345678-1234-5678-1234-567812345678")
TOP_N = 5
MAX_CHUNK_LENGTH = 300

# âœ… è·å–å‘é‡ï¼ˆé€šä¹‰ DashScope æ­£ç¡®æ¥å£ï¼‰
def get_embedding(text: str) -> list[float]:
    logger.info(f"ğŸ” å‘é‡åŒ–æ–‡æœ¬é•¿åº¦: {len(text)}")
    try:
        response = TextEmbedding.call(
            model=DASHSCOPE_EMBEDDING_MODEL,
            input=text
        )
        # æ£€æŸ¥ response ç»“æ„
        if response and hasattr(response, "output") and response.output:
            embedding = response.output.get("embeddings", [{}])[0].get("embedding")
            if embedding:
                logger.info("âœ… å‘é‡åŒ–æˆåŠŸï¼å‰5ç»´å‘é‡ä¸º: %s", embedding[:5])
                return embedding
        logger.error(f"âŒ é€šä¹‰è¿”å›ç»“æœå¼‚å¸¸: {response}")
        return []
    except Exception as e:
        logger.error(f"âŒ è·å–å‘é‡å¤±è´¥: {str(e)}")
        return []

# âœ… å®‰å…¨æˆªæ–­æ–‡æœ¬
def safe_truncate(text: str, max_length: int = 1000) -> str:
    return text[:max_length] if len(text) > max_length else text

# âœ… åˆ‡åˆ†æ–‡æœ¬æ®µè½
def chunk_text(text: str, max_length: int) -> List[str]:
    sentences = re.split(r'[\n\u3002\uff01\uff1f!\?]', text)
    chunks, current = [], ""
    for sentence in sentences:
        if len(current) + len(sentence) < max_length:
            current += sentence + "ã€‚"
        else:
            if current:
                chunks.append(current.strip())
            current = sentence + "ã€‚"
    if current:
        chunks.append(current.strip())
    return [c for c in chunks if len(c) > 10]

# âœ… ä» .txt ä¸­æå–ç»“æ„å­—æ®µã€æ¨¡å—å†…å®¹å’ŒåŸæ–‡éƒ¨åˆ†
def split_txt_sections(full_text: str):
    pattern = r"===\s*å­—æ®µæå–ç»“æœ\s*===\n(.*?)\n+===\s*æ¨¡å—ç»“æ„åˆ†ç±»ç»“æœ\s*===\n(.*?)\n+===\s*åŸå§‹ç®€å†æ–‡æœ¬\s*===\n(.*)"
    match = re.search(pattern, full_text, re.DOTALL)
    if match:
        fields_str, sections_str, raw_text = match.groups()
        return fields_str.strip(), sections_str.strip(), raw_text.strip()
    else:
        return "", "", full_text.strip()

# âœ… æ‹¼æ¥ç»“æ„ä¿¡æ¯ä¸åŸæ–‡æ–‡æœ¬ç”¨äºå‘é‡åŒ–
def build_vector_text(fields_str: str, sections_str: str, raw_text: str) -> str:
    return f"""ã€å­—æ®µä¿¡æ¯ã€‘\n{fields_str}\n\nã€æ¨¡å—ç»“æ„åˆ†ç±»ã€‘\n{sections_str}\n\nã€åŸæ–‡è¡¥å……ã€‘\n{raw_text}"""

# âœ… è¯»å–ç®€å†æ–‡æœ¬
def load_resume_text(file_path: str) -> str:
    if file_path.endswith(".txt"):
        with open(file_path, 'r', encoding="utf-8") as f:
            return f.read()
    return ""

# âœ… æ£€æŸ¥ Weaviate æ˜¯å¦å·²æœ‰è¯¥å¯¹è±¡
def object_exists(resume_uuid: str) -> bool:
    url = f"{WEAVIATE_URL}/v1/objects/{WEAVIATE_CLASS}/{resume_uuid}"
    response = requests.get(url)
    return response.status_code == 200

# âœ… ä¸Šä¼ åˆ° Weaviate
def upload_resume(filename: str, content: str, vector: List[float], resume_uuid: str):
    if vector is None or not isinstance(vector, list):
        raise ValueError("âŒ å‘é‡ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼")

    payload = {
        "class": WEAVIATE_CLASS,
        "id": resume_uuid,
        "properties": {
            "filename": filename,
            "content": content,
            "notes": []
        },
        "vector": vector
    }

    logger.debug("ğŸ“¦ ä¸Šä¼ å¯¹è±¡ payload: \n%s", json.dumps(payload, indent=2, ensure_ascii=False))

    url = f"{WEAVIATE_URL}/v1/objects"
    response = requests.post(url, json=payload)
    if response.status_code == 200:
        logger.info("âœ… æ’å…¥æˆåŠŸ: %s", filename)
    else:
        logger.error("âŒ æ’å…¥å¤±è´¥: %s", filename)
        logger.error("çŠ¶æ€ç : %d", response.status_code)
        try:
            logger.error("é”™è¯¯ä¿¡æ¯: %s", response.json())
        except:
            logger.warning("âš ï¸ å“åº”ä½“è§£æå¤±è´¥")

# âœ… æ ¹æ®æ–‡ä»¶åå’Œå†…å®¹ç”Ÿæˆ UUID
def generate_uuid_from_file(filename: str, content: str) -> str:
    hash_val = hashlib.sha256(content.encode('utf-8')).hexdigest()
    return str(uuid.uuid5(NAMESPACE_UUID, filename + hash_val))

# âœ… ä¸»é€»è¾‘
def index_resumes_topn():
    logger.info("ğŸ”§ å½“å‰ä½¿ç”¨é€šä¹‰æ¨¡å‹: %s", DASHSCOPE_EMBEDDING_MODEL)
    if not os.path.exists(RESUMES_DIR):
        logger.error("âŒ ç®€å†ç›®å½•ä¸å­˜åœ¨: %s", RESUMES_DIR)
        return

    files = [f for f in os.listdir(RESUMES_DIR) if f.endswith(".txt")]
    if not files:
        logger.warning("âš ï¸ æ²¡æœ‰å‘ç°ç®€å†æ–‡ä»¶")
        return

    logger.info("ğŸ“„ å¼€å§‹å¤„ç†ç®€å†:")
    start = time.time()

    for filename in tqdm(files):
        file_path = os.path.join(RESUMES_DIR, filename)
        full_text = load_resume_text(file_path)
        if not full_text.strip():
            logger.warning("âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶: %s", filename)
            continue

        fields, sections, raw = split_txt_sections(full_text)
        merged_text = build_vector_text(fields, sections, raw)
        chunks = chunk_text(merged_text, MAX_CHUNK_LENGTH)
        selected_chunks = chunks[:TOP_N]

        if not selected_chunks:
            logger.warning("âš ï¸ æ— æœ‰æ•ˆæ®µè½: %s", filename)
            continue

        final_text = safe_truncate("\n".join(selected_chunks))
        resume_uuid = generate_uuid_from_file(filename, final_text)

        if object_exists(resume_uuid):
            logger.info("â© å·²å­˜åœ¨: %s", filename)
            continue

        try:
            vector = get_embedding(final_text)
            upload_resume(filename, final_text, vector, resume_uuid)
            time.sleep(0.3)
        except Exception as e:
            logger.exception("âŒ ä¸Šä¼ å¤±è´¥: %s", filename)

    elapsed = time.time() - start
    logger.info("âœ… æ‰€æœ‰ç®€å†å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ %.2fs", elapsed)

if __name__ == "__main__":
    index_resumes_topn()