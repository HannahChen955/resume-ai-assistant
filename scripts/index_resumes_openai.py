#!/usr/bin/env python3

import os
import uuid
import time
import json
import requests
import re
from typing import List
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv

# âœ… åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# âœ… ç¯å¢ƒå˜é‡é…ç½®
RESUMES_DIR = os.getenv("EXTRACTED_DIR", "data/resumes_extract_enhanced")
WEAVIATE_URL = os.getenv("WEAVIATE_URL", "http://localhost:8080")
WEAVIATE_CLASS = os.getenv("WEAVIATE_COLLECTION", "Candidates")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")
NAMESPACE_UUID = uuid.UUID("12345678-1234-5678-1234-567812345678")
TOP_N = 5
MAX_CHUNK_LENGTH = 300
LLM_MODEL = os.getenv("OPENAI_LLM_FIELD_EXTRACT_MODEL", "gpt-3.5-turbo")

# âœ… åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… è·å–å‘é‡
def get_embedding(text: str) -> List[float]:
    response = openai_client.embeddings.create(
        input=[text],
        model=EMBEDDING_MODEL
    )
    return response.data[0].embedding

# âœ… GPT åˆ†æ®µæ‰“åˆ†
def score_chunk(query: str, chunk: str) -> float:
    prompt = f"è¯·æ ¹æ®ä¸ {query} å²—ä½çš„ç›¸å…³æ€§ï¼Œå¯¹ä»¥ä¸‹æ–‡æœ¬æ‰“åˆ†ï¼ˆæ»¡åˆ†100åˆ†ï¼‰ï¼Œåªè¿”å›æ•°å­—ï¼š\n\n{chunk}"
    response = openai_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    try:
        return float(response.choices[0].message.content.strip().split()[0])
    except:
        return 0.0

# âœ… åˆ‡åˆ†æ–‡æœ¬æ®µè½
def chunk_text(text: str, max_length: int) -> List[str]:
    sentences = re.split(r'[\n\u3002\uff01\uff1f!\?]', text)
    chunks, current = [], ""
    for sentence in sentences:
        if len(current) + len(sentence) < max_length:
            current += sentence + "ã€‚"
        else:
            if current:
                chunks.append(current.strip())
            current = sentence + "ã€‚"
    if current:
        chunks.append(current.strip())
    return [c for c in chunks if len(c) > 10]

# âœ… ä» .txt ä¸­æå–ç»“æ„å­—æ®µã€æ¨¡å—å†…å®¹å’ŒåŸæ–‡éƒ¨åˆ†
def split_txt_sections(full_text: str):
    pattern = r"===\s*å­—æ®µæå–ç»“æœ\s*===\n(.*?)\n+===\s*æ¨¡å—ç»“æ„åˆ†ç±»ç»“æœ\s*===\n(.*?)\n+===\s*åŸå§‹ç®€å†æ–‡æœ¬\s*===\n(.*)"
    match = re.search(pattern, full_text, re.DOTALL)
    if match:
        fields_str, sections_str, raw_text = match.groups()
        return fields_str.strip(), sections_str.strip(), raw_text.strip()
    else:
        return "", "", full_text.strip()

# âœ… æ‹¼æ¥ç»“æ„ä¿¡æ¯ä¸åŸæ–‡æ–‡æœ¬ç”¨äºå‘é‡åŒ–
def build_vector_text(fields_str: str, sections_str: str, raw_text: str) -> str:
    return f"""ã€å­—æ®µä¿¡æ¯ã€‘\n{fields_str}\n\nã€æ¨¡å—ç»“æ„åˆ†ç±»ã€‘\n{sections_str}\n\nã€åŸæ–‡è¡¥å……ã€‘\n{raw_text}"""

# âœ… è¯»å–ç®€å†æ–‡æœ¬
def load_resume_text(file_path: str) -> str:
    if file_path.endswith(".txt"):
        with open(file_path, 'r', encoding="utf-8") as f:
            return f.read()
    return ""

# âœ… æ£€æŸ¥ Weaviate æ˜¯å¦å·²æœ‰è¯¥å¯¹è±¡
def object_exists(resume_uuid: str) -> bool:
    url = f"{WEAVIATE_URL}/v1/objects/{WEAVIATE_CLASS}/{resume_uuid}"
    response = requests.get(url)
    return response.status_code == 200

# âœ… ä¸Šä¼ åˆ° Weaviate
def upload_resume(filename: str, content: str, vector: List[float], resume_uuid: str):
    payload = {
        "class": WEAVIATE_CLASS,
        "id": resume_uuid,
        "properties": {
            "filename": filename,
            "content": content,
            "notes": []
        },
        "vector": vector
    }
    url = f"{WEAVIATE_URL}/v1/objects"
    response = requests.post(url, json=payload)
    if response.status_code == 200:
        print(f"âœ… æ’å…¥æˆåŠŸ: {filename}")
    else:
        print(f"âŒ æ’å…¥å¤±è´¥: {filename}")
        print("çŠ¶æ€ç :", response.status_code)
        try:
            print("é”™è¯¯ä¿¡æ¯:", response.json())
        except:
            print("âš ï¸ å“åº”ä½“è§£æå¤±è´¥")

# âœ… ä¸»é€»è¾‘
def index_resumes_topn():
    print(f"ğŸ”§ å½“å‰ä½¿ç”¨ embedding æ¨¡å‹: {EMBEDDING_MODEL}")
    if not os.path.exists(RESUMES_DIR):
        print(f"âŒ ç®€å†ç›®å½•ä¸å­˜åœ¨: {RESUMES_DIR}")
        return

    files = [f for f in os.listdir(RESUMES_DIR) if f.endswith(".txt")]
    if not files:
        print("âš ï¸ æ²¡æœ‰å‘ç°ç®€å†æ–‡ä»¶")
        return

    print("ğŸ“„ å¼€å§‹å¤„ç†ç®€å†:")

    for filename in tqdm(files):
        file_path = os.path.join(RESUMES_DIR, filename)
        full_text = load_resume_text(file_path)
        if not full_text.strip():
            print(f"âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶: {filename}")
            continue

        fields, sections, raw = split_txt_sections(full_text)
        merged_text = build_vector_text(fields, sections, raw)
        chunks = chunk_text(merged_text, MAX_CHUNK_LENGTH)
        scored = [(chunk, score_chunk("å…‰å­¦å·¥ç¨‹å¸ˆ", chunk)) for chunk in chunks]
        sorted_chunks = sorted(scored, key=lambda x: x[1], reverse=True)
        selected_chunks = [chunk for chunk, _ in sorted_chunks[:TOP_N]]

        if not selected_chunks:
            print(f"âš ï¸ æ— æœ‰æ•ˆæ®µè½: {filename}")
            continue

        final_text = "\n".join(selected_chunks)
        resume_uuid = str(uuid.uuid5(NAMESPACE_UUID, filename))

        if object_exists(resume_uuid):
            print(f"â© å·²å­˜åœ¨: {filename}")
            continue

        try:
            vector = get_embedding(final_text)
            upload_resume(filename, final_text, vector, resume_uuid)
            time.sleep(0.3)
        except Exception as e:
            import traceback
            print(f"âŒ ä¸Šä¼ å¤±è´¥: {filename}")
            traceback.print_exc()

    print("âœ… æ‰€æœ‰ç®€å†å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    index_resumes_topn()
